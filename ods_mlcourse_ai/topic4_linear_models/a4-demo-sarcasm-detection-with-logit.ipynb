{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose.","metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"}},{"cell_type":"markdown","source":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"}},{"cell_type":"code","source":"!ls ../input/sarcasm/","metadata":{"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247","execution":{"iopub.status.busy":"2021-12-07T16:22:25.842184Z","iopub.execute_input":"2021-12-07T16:22:25.843426Z","iopub.status.idle":"2021-12-07T16:22:26.658725Z","shell.execute_reply.started":"2021-12-07T16:22:25.843343Z","shell.execute_reply":"2021-12-07T16:22:26.657720Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.status.busy":"2021-12-07T16:22:26.659929Z","iopub.execute_input":"2021-12-07T16:22:26.660173Z","iopub.status.idle":"2021-12-07T16:22:26.666525Z","shell.execute_reply.started":"2021-12-07T16:22:26.660136Z","shell.execute_reply":"2021-12-07T16:22:26.665461Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"SEED = 42","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:26.667607Z","iopub.execute_input":"2021-12-07T16:22:26.668017Z","iopub.status.idle":"2021-12-07T16:22:26.682930Z","shell.execute_reply.started":"2021-12-07T16:22:26.667969Z","shell.execute_reply":"2021-12-07T16:22:26.681827Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.status.busy":"2021-12-07T16:22:26.684154Z","iopub.execute_input":"2021-12-07T16:22:26.684539Z","iopub.status.idle":"2021-12-07T16:22:32.902814Z","shell.execute_reply.started":"2021-12-07T16:22:26.684501Z","shell.execute_reply":"2021-12-07T16:22:32.901764Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.status.busy":"2021-12-07T16:22:32.903851Z","iopub.execute_input":"2021-12-07T16:22:32.904262Z","iopub.status.idle":"2021-12-07T16:22:32.938525Z","shell.execute_reply.started":"2021-12-07T16:22:32.904212Z","shell.execute_reply":"2021-12-07T16:22:32.937584Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.status.busy":"2021-12-07T16:22:32.940070Z","iopub.execute_input":"2021-12-07T16:22:32.940415Z","iopub.status.idle":"2021-12-07T16:22:33.754786Z","shell.execute_reply.started":"2021-12-07T16:22:32.940350Z","shell.execute_reply":"2021-12-07T16:22:33.753912Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows.","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"}},{"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.status.busy":"2021-12-07T16:22:33.756335Z","iopub.execute_input":"2021-12-07T16:22:33.756707Z","iopub.status.idle":"2021-12-07T16:22:34.056366Z","shell.execute_reply.started":"2021-12-07T16:22:33.756633Z","shell.execute_reply":"2021-12-07T16:22:34.055358Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"We notice that the dataset is indeed balanced","metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"}},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11","execution":{"iopub.status.busy":"2021-12-07T16:22:34.058134Z","iopub.execute_input":"2021-12-07T16:22:34.058524Z","iopub.status.idle":"2021-12-07T16:22:34.078685Z","shell.execute_reply.started":"2021-12-07T16:22:34.058452Z","shell.execute_reply":"2021-12-07T16:22:34.077590Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"We split data into training and validation parts.","metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"}},{"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = train_test_split(train_df, train_df['label'], random_state=17)","metadata":{"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96","execution":{"iopub.status.busy":"2021-12-07T16:22:34.080178Z","iopub.execute_input":"2021-12-07T16:22:34.080530Z","iopub.status.idle":"2021-12-07T16:22:35.357138Z","shell.execute_reply.started":"2021-12-07T16:22:34.080460Z","shell.execute_reply":"2021-12-07T16:22:35.356137Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at the dataset (head, info, describe, columns) ","metadata":{}},{"cell_type":"markdown","source":"For subsequent analysis I will use only (train_texts, y_train). </br>\nLet's look at the data by printing first 10 rows and descriptive statistics","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 Let's look at the data by printing its head","metadata":{}},{"cell_type":"code","source":"train_texts.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:35.358727Z","iopub.execute_input":"2021-12-07T16:22:35.359115Z","iopub.status.idle":"2021-12-07T16:22:35.394659Z","shell.execute_reply.started":"2021-12-07T16:22:35.359028Z","shell.execute_reply":"2021-12-07T16:22:35.393700Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Descriptive statistics","metadata":{}},{"cell_type":"code","source":"train_texts.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:35.396164Z","iopub.execute_input":"2021-12-07T16:22:35.396518Z","iopub.status.idle":"2021-12-07T16:22:42.853102Z","shell.execute_reply.started":"2021-12-07T16:22:35.396457Z","shell.execute_reply":"2021-12-07T16:22:42.852209Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Columns","metadata":{}},{"cell_type":"markdown","source":"As soon as we deleted rows with omitted comments we don't have missing values in the dataset.\n* **label** - label 0/1 - non-sarcastic/sarcastic\n* **comment** - comment itself. Later we will be able to predict which of them are sarcastic and which are not. </br>\n    * find the number of words in each sentence and compare length distributions for both sarcastic and non-sarcastic comments\n    * score & length & label interaction\n* **author** - author's nickname\n    * find unique values \n    * find counts\n* **subreddit** - name of the subreddit. A subreddit is a specific online community, and the posts associated with it, on the social media website Reddit. Subreddits are dedicated to a particular topic that people write about, and they’re denoted by /r/, followed by the subreddit’s name, e.g., /r/gaming.\n    * find unique values\n    * calculate the \"sarcasticness\" of the top subreddits\n* **score** - comment's score. Which is simply the number of upvotes minus the number of downvotes.\n* **ups** and **downs** - number of ups and downs respectively\n    * look for the errors in the data (for example, min value for ups is -261, which is strange)\n* **date** - year and month when the comment was written\n* **created_utc** - date + day, hours, minutes, secons\n    * dependency between day of the week and/or hour of the day and the sarcasm?\n* **parent_comment**","metadata":{}},{"cell_type":"markdown","source":"#### 1.4 Data instabilities","metadata":{}},{"cell_type":"code","source":"correct_score = sum(train_texts['score'] == train_texts['ups'] - train_texts['downs'])\nprint(f\"The score is correct only for {correct_score} number of rows out of {train_texts.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:42.854875Z","iopub.execute_input":"2021-12-07T16:22:42.855176Z","iopub.status.idle":"2021-12-07T16:22:42.905689Z","shell.execute_reply.started":"2021-12-07T16:22:42.855103Z","shell.execute_reply":"2021-12-07T16:22:42.904662Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Data instabilities noticed: </br>\n* score, ups and downs behave strangely. Score must be equal to ups - downs","metadata":{}},{"cell_type":"markdown","source":"#### 1.5 Dealing with missing data and outliers","metadata":{}},{"cell_type":"markdown","source":"Currently, this is out of scope","metadata":{}},{"cell_type":"markdown","source":"### 2. Target variable analysis","metadata":{}},{"cell_type":"code","source":"sns.countplot('label',data=train_texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:42.906908Z","iopub.execute_input":"2021-12-07T16:22:42.907153Z","iopub.status.idle":"2021-12-07T16:22:43.227956Z","shell.execute_reply.started":"2021-12-07T16:22:42.907110Z","shell.execute_reply":"2021-12-07T16:22:43.226806Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"The train dataset is balanced, thus we can use accuracy score without a second thought","metadata":{}},{"cell_type":"markdown","source":"### 3. Feature Analysis","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Looking at columns and determining feature types ","metadata":{}},{"cell_type":"code","source":"train_texts.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:43.229571Z","iopub.execute_input":"2021-12-07T16:22:43.230185Z","iopub.status.idle":"2021-12-07T16:22:44.158800Z","shell.execute_reply.started":"2021-12-07T16:22:43.230121Z","shell.execute_reply":"2021-12-07T16:22:44.157469Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"We have 10 columns: 1 label, 3 numerical (score, ups, downs) and 6 strings/timstamps. ","metadata":{}},{"cell_type":"markdown","source":"#### 3.2 Summarizing data and showing some statistics:","metadata":{}},{"cell_type":"code","source":"train_texts.describe(include=[\"object\", \"bool\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:44.160466Z","iopub.execute_input":"2021-12-07T16:22:44.160850Z","iopub.status.idle":"2021-12-07T16:22:51.277143Z","shell.execute_reply.started":"2021-12-07T16:22:44.160788Z","shell.execute_reply":"2021-12-07T16:22:51.276267Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"Some thoughts:\n* Almost all the comments are unique\n* One third of authors are unique\n* Approximatelly 13 000 subreddits are in the dataset\n* Datasets spans 96 months = 8 years of observations","metadata":{}},{"cell_type":"code","source":"for label, dataset in train_texts.groupby('label'):\n    print(f\"\\nFor label {label} the object data statistics is:\\n\")\n    print(dataset.describe(include=['object', 'bool']))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:51.279159Z","iopub.execute_input":"2021-12-07T16:22:51.279543Z","iopub.status.idle":"2021-12-07T16:22:58.911845Z","shell.execute_reply.started":"2021-12-07T16:22:51.279477Z","shell.execute_reply":"2021-12-07T16:22:58.910823Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Separate statistics for 0 and 1 labels doesn't look different","metadata":{}},{"cell_type":"code","source":"train_texts.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:58.913770Z","iopub.execute_input":"2021-12-07T16:22:58.914136Z","iopub.status.idle":"2021-12-07T16:22:59.163445Z","shell.execute_reply.started":"2021-12-07T16:22:58.914067Z","shell.execute_reply":"2021-12-07T16:22:59.162292Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3 Analysis","metadata":{}},{"cell_type":"markdown","source":"##### 3.3.1 Comment","metadata":{}},{"cell_type":"markdown","source":"Let's add new variable which will denote the length of the comment","metadata":{}},{"cell_type":"code","source":"train_texts['length'] = [len(comment.split()) for comment in train_texts['comment']]","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:22:59.165050Z","iopub.execute_input":"2021-12-07T16:22:59.165336Z","iopub.status.idle":"2021-12-07T16:23:00.759205Z","shell.execute_reply.started":"2021-12-07T16:22:59.165285Z","shell.execute_reply":"2021-12-07T16:23:00.758081Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"train_texts[['comment', 'length']].head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:00.760522Z","iopub.execute_input":"2021-12-07T16:23:00.760825Z","iopub.status.idle":"2021-12-07T16:23:00.825843Z","shell.execute_reply.started":"2021-12-07T16:23:00.760775Z","shell.execute_reply":"2021-12-07T16:23:00.824874Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"train_texts['length'].describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:00.827235Z","iopub.execute_input":"2021-12-07T16:23:00.827480Z","iopub.status.idle":"2021-12-07T16:23:00.897071Z","shell.execute_reply.started":"2021-12-07T16:23:00.827436Z","shell.execute_reply":"2021-12-07T16:23:00.896052Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"print(f\"0.025 quantile of comment's length is equal to {train_texts['length'].quantile(0.025)}\")\nprint(f\"0.975 quantile of comment's length is equal to {train_texts['length'].quantile(0.975)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:00.898768Z","iopub.execute_input":"2021-12-07T16:23:00.899043Z","iopub.status.idle":"2021-12-07T16:23:00.931927Z","shell.execute_reply.started":"2021-12-07T16:23:00.898997Z","shell.execute_reply":"2021-12-07T16:23:00.930813Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have some outliers in the length of the comment column. </br>\nFor the purpose of visualization I'm going to filter out some outliers. In future whether it will be helpful for classification or not","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts = train_texts[train_texts['length'] <= train_texts['length'].quantile(0.975)] ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:00.933172Z","iopub.execute_input":"2021-12-07T16:23:00.933450Z","iopub.status.idle":"2021-12-07T16:23:01.215029Z","shell.execute_reply.started":"2021-12-07T16:23:00.933405Z","shell.execute_reply":"2021-12-07T16:23:01.214025Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"Now we can compare the length distributions for 0/1 labels","metadata":{}},{"cell_type":"code","source":"g = sns.kdeplot(cleaned_train_texts[cleaned_train_texts[\"label\"]==0][\"length\"], color=\"red\", shade=True)\ng = sns.kdeplot(cleaned_train_texts[cleaned_train_texts[\"label\"]==1][\"length\"], color=\"blue\", shade=True)\ng = g.legend([\"not sarcasm\", \"sarcasm\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:01.216564Z","iopub.execute_input":"2021-12-07T16:23:01.216841Z","iopub.status.idle":"2021-12-07T16:23:01.935324Z","shell.execute_reply.started":"2021-12-07T16:23:01.216796Z","shell.execute_reply":"2021-12-07T16:23:01.934136Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"cleaned_train_texts[cleaned_train_texts[\"label\"]==0][\"length\"].plot.hist(bins=50, alpha = 0.5, edgecolor=\"black\", color=\"red\")\ncleaned_train_texts[cleaned_train_texts[\"label\"]==1][\"length\"].plot.hist(bins=50, alpha = 0.5, edgecolor=\"black\", color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:01.937203Z","iopub.execute_input":"2021-12-07T16:23:01.937968Z","iopub.status.idle":"2021-12-07T16:23:02.822820Z","shell.execute_reply.started":"2021-12-07T16:23:01.937896Z","shell.execute_reply":"2021-12-07T16:23:02.821715Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"'length' feature may not be the informative one. But maybe we can split it into bins - into three intervals: [1, 5), [5, 20), [21, 30] and it will improve the accuracy score.","metadata":{}},{"cell_type":"markdown","source":"##### 3.3.2 Score","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts['score'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:02.824974Z","iopub.execute_input":"2021-12-07T16:23:02.825753Z","iopub.status.idle":"2021-12-07T16:23:02.901497Z","shell.execute_reply.started":"2021-12-07T16:23:02.825658Z","shell.execute_reply":"2021-12-07T16:23:02.900636Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"'score' feature also has outliers. We ought to get rid of them","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts = cleaned_train_texts[(cleaned_train_texts['score'] <= cleaned_train_texts['score'].quantile(0.975)) & \n                                  (cleaned_train_texts['score'] >= cleaned_train_texts['score'].quantile(0.025))] ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:02.902708Z","iopub.execute_input":"2021-12-07T16:23:02.902981Z","iopub.status.idle":"2021-12-07T16:23:03.198682Z","shell.execute_reply.started":"2021-12-07T16:23:02.902938Z","shell.execute_reply":"2021-12-07T16:23:03.197772Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"cleaned_train_texts[(cleaned_train_texts[\"label\"]==0)][\"score\"].plot.hist(bins=20, alpha=0.5, edgecolor=\"black\", color=\"red\")\ncleaned_train_texts[(cleaned_train_texts[\"label\"]==1)][\"score\"].plot.hist(bins=20, alpha=0.5, edgecolor=\"black\", color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:03.200679Z","iopub.execute_input":"2021-12-07T16:23:03.201031Z","iopub.status.idle":"2021-12-07T16:23:03.911023Z","shell.execute_reply.started":"2021-12-07T16:23:03.200961Z","shell.execute_reply":"2021-12-07T16:23:03.909714Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"'score' feature also seems to have minor effect on the value of the 'label' column","metadata":{}},{"cell_type":"markdown","source":"##### 3.3.3 Subreddits","metadata":{}},{"cell_type":"code","source":"subreddit_unique = train_texts['subreddit'].unique()\nprint(len(subreddit_unique))","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:03.913253Z","iopub.execute_input":"2021-12-07T16:23:03.914153Z","iopub.status.idle":"2021-12-07T16:23:04.023652Z","shell.execute_reply.started":"2021-12-07T16:23:03.914075Z","shell.execute_reply":"2021-12-07T16:23:04.022516Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"The number of unique subreddits is 13254\n","metadata":{}},{"cell_type":"code","source":"top_subreddits = pd.DataFrame(\n    train_texts.groupby('subreddit')['label'].agg(['count', 'mean'])\n                                             .round(2)\n                                             .sort_values(by='count', ascending=False)\n                                             .head(200)).rename(columns={\"mean\":\"sarcasticness\"})","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:04.025526Z","iopub.execute_input":"2021-12-07T16:23:04.026194Z","iopub.status.idle":"2021-12-07T16:23:04.178895Z","shell.execute_reply.started":"2021-12-07T16:23:04.025915Z","shell.execute_reply":"2021-12-07T16:23:04.177797Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"top_subreddits.T","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:04.180325Z","iopub.execute_input":"2021-12-07T16:23:04.180647Z","iopub.status.idle":"2021-12-07T16:23:04.337112Z","shell.execute_reply.started":"2021-12-07T16:23:04.180575Z","shell.execute_reply":"2021-12-07T16:23:04.336316Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"We can see that the percentage of sarcastic comments differs from subreddit to subreddit. </br>\nThis feature can be useful in prediction.","metadata":{}},{"cell_type":"markdown","source":"### 4. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"This step will be skipped right now to build baseline model. After the estimation of baseline model performance we will return to feature engineering","metadata":{}},{"cell_type":"markdown","source":"### 5. Predictive modelling","metadata":{}},{"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = train_test_split(train_df['comment'], train_df['label'], random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:04.338184Z","iopub.execute_input":"2021-12-07T16:23:04.338593Z","iopub.status.idle":"2021-12-07T16:23:04.709948Z","shell.execute_reply.started":"2021-12-07T16:23:04.338534Z","shell.execute_reply":"2021-12-07T16:23:04.708763Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"tf_idf = TfidfVectorizer()\nlog_reg = LogisticRegression(random_state=SEED)\npipeline = Pipeline([('tf_idf', tf_idf), ('log_reg', log_reg)])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:04.712801Z","iopub.execute_input":"2021-12-07T16:23:04.713499Z","iopub.status.idle":"2021-12-07T16:23:04.719506Z","shell.execute_reply.started":"2021-12-07T16:23:04.713429Z","shell.execute_reply":"2021-12-07T16:23:04.718727Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"pipeline.fit(train_texts, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:23:04.720709Z","iopub.execute_input":"2021-12-07T16:23:04.721199Z","iopub.status.idle":"2021-12-07T16:23:43.372044Z","shell.execute_reply.started":"2021-12-07T16:23:04.721138Z","shell.execute_reply":"2021-12-07T16:23:43.371040Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"y_pred = pipeline.predict(valid_texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:24:03.180383Z","iopub.execute_input":"2021-12-07T16:24:03.181132Z","iopub.status.idle":"2021-12-07T16:24:08.568998Z","shell.execute_reply.started":"2021-12-07T16:24:03.181079Z","shell.execute_reply":"2021-12-07T16:24:08.567834Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"acc_baseline = accuracy_score(y_pred=y_pred, y_true=y_valid)\nprint(f\"Accuracy for baseline model is {acc_baseline.round(2)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:24:08.570842Z","iopub.execute_input":"2021-12-07T16:24:08.571241Z","iopub.status.idle":"2021-12-07T16:24:08.606795Z","shell.execute_reply.started":"2021-12-07T16:24:08.571169Z","shell.execute_reply":"2021-12-07T16:24:08.606011Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"Accuracy for baseline model is 0.69. The datset for training was taken as it is. Hyperparameters weren't tuned.\n","metadata":{}},{"cell_type":"code","source":"pipeline.fit(cleaned_train_texts['comment'], cleaned_train_texts['label'])\ny_pred_cleaned = pipeline.predict(valid_texts)\nacc_cleaned = accuracy_score(y_pred=y_pred_cleaned, y_true=y_valid)\nprint(f\"Accuracy for baseline model with cleaned dataset is {acc_cleaned.round(2)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T16:26:43.577502Z","iopub.execute_input":"2021-12-07T16:26:43.577976Z","iopub.status.idle":"2021-12-07T16:27:20.143138Z","shell.execute_reply.started":"2021-12-07T16:26:43.577900Z","shell.execute_reply":"2021-12-07T16:27:20.142380Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions","metadata":{"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"}}]}