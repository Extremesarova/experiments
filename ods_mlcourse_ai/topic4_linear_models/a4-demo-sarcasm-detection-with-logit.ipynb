{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) – Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose.","metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"}},{"cell_type":"markdown","source":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"}},{"cell_type":"code","source":"!ls ../input/sarcasm/","metadata":{"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247","execution":{"iopub.status.busy":"2021-12-06T17:01:56.472769Z","iopub.execute_input":"2021-12-06T17:01:56.473342Z","iopub.status.idle":"2021-12-06T17:01:57.251784Z","shell.execute_reply.started":"2021-12-06T17:01:56.473238Z","shell.execute_reply":"2021-12-06T17:01:57.250764Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.status.busy":"2021-12-06T17:01:57.254053Z","iopub.execute_input":"2021-12-06T17:01:57.254500Z","iopub.status.idle":"2021-12-06T17:01:58.825406Z","shell.execute_reply.started":"2021-12-06T17:01:57.254438Z","shell.execute_reply":"2021-12-06T17:01:58.824391Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.status.busy":"2021-12-06T17:01:58.826644Z","iopub.execute_input":"2021-12-06T17:01:58.826880Z","iopub.status.idle":"2021-12-06T17:02:06.541832Z","shell.execute_reply.started":"2021-12-06T17:01:58.826841Z","shell.execute_reply":"2021-12-06T17:02:06.541064Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.status.busy":"2021-12-06T17:02:06.543237Z","iopub.execute_input":"2021-12-06T17:02:06.543536Z","iopub.status.idle":"2021-12-06T17:02:06.594012Z","shell.execute_reply.started":"2021-12-06T17:02:06.543486Z","shell.execute_reply":"2021-12-06T17:02:06.593005Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.status.busy":"2021-12-06T17:02:06.595470Z","iopub.execute_input":"2021-12-06T17:02:06.595715Z","iopub.status.idle":"2021-12-06T17:02:07.443291Z","shell.execute_reply.started":"2021-12-06T17:02:06.595669Z","shell.execute_reply":"2021-12-06T17:02:07.442559Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows.","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"}},{"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.status.busy":"2021-12-06T17:02:07.445009Z","iopub.execute_input":"2021-12-06T17:02:07.445587Z","iopub.status.idle":"2021-12-06T17:02:07.764145Z","shell.execute_reply.started":"2021-12-06T17:02:07.445443Z","shell.execute_reply":"2021-12-06T17:02:07.763312Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We notice that the dataset is indeed balanced","metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"}},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11","execution":{"iopub.status.busy":"2021-12-06T17:02:07.765659Z","iopub.execute_input":"2021-12-06T17:02:07.765921Z","iopub.status.idle":"2021-12-06T17:02:07.784675Z","shell.execute_reply.started":"2021-12-06T17:02:07.765870Z","shell.execute_reply":"2021-12-06T17:02:07.783883Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We split data into training and validation parts.","metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"}},{"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = train_test_split(train_df, train_df['label'], random_state=17)","metadata":{"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96","execution":{"iopub.status.busy":"2021-12-06T17:02:07.788210Z","iopub.execute_input":"2021-12-06T17:02:07.788463Z","iopub.status.idle":"2021-12-06T17:02:08.858736Z","shell.execute_reply.started":"2021-12-06T17:02:07.788412Z","shell.execute_reply":"2021-12-06T17:02:08.857645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at the dataset (head, info, describe, columns) ","metadata":{}},{"cell_type":"markdown","source":"For subsequent analysis I will use only (train_texts, y_train). </br>\nLet's look at the data by printing first 10 rows and descriptive statistics","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 Let's look at the data by printing its head","metadata":{}},{"cell_type":"code","source":"train_texts.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:02:08.859942Z","iopub.execute_input":"2021-12-06T17:02:08.860239Z","iopub.status.idle":"2021-12-06T17:02:08.891981Z","shell.execute_reply.started":"2021-12-06T17:02:08.860175Z","shell.execute_reply":"2021-12-06T17:02:08.890895Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 Descriptive statistics","metadata":{}},{"cell_type":"code","source":"train_texts.describe(include='all').T","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:02:08.893793Z","iopub.execute_input":"2021-12-06T17:02:08.894110Z","iopub.status.idle":"2021-12-06T17:02:14.998203Z","shell.execute_reply.started":"2021-12-06T17:02:08.894037Z","shell.execute_reply":"2021-12-06T17:02:14.997496Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 Columns","metadata":{}},{"cell_type":"markdown","source":"As soon as we deleted rows with omitted comments we don't have missing values in the dataset.\n* **label** - label 0/1 - non-sarcastic/sarcastic\n* **comment** - comment itself. Later we will be able to predict which of them are sarcastic and which are not. </br>\n    * find the number of words in each sentence and compare length distributions for both sarcastic and non-sarcastic comments\n    * score & length & label interaction\n* **author** - author's nickname\n    * find unique values \n    * find counts\n* **subreddit** - name of the subreddit. A subreddit is a specific online community, and the posts associated with it, on the social media website Reddit. Subreddits are dedicated to a particular topic that people write about, and they’re denoted by /r/, followed by the subreddit’s name, e.g., /r/gaming.\n    * find unique values\n    * calculate the \"sarcasticness\" of the top subreddits\n* **score** - comment's score. Which is simply the number of upvotes minus the number of downvotes.\n* **ups** and **downs** - number of ups and downs respectively\n    * look for the errors in the data (for example, min value for ups is -261, which is strange)\n* **date** - year and month when the comment was written\n* **created_utc** - date + day, hours, minutes, secons\n    * dependency between day of the week and/or hour of the day and the sarcasm?\n* **parent_comment**","metadata":{}},{"cell_type":"markdown","source":"#### 1.4 Data instabilities","metadata":{}},{"cell_type":"code","source":"correct_score = sum(train_texts['score'] == train_texts['ups'] - train_texts['downs'])\nprint(f\"The score is correct only for {correct_score} number of rows out of {train_texts.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:02:14.999241Z","iopub.execute_input":"2021-12-06T17:02:14.999643Z","iopub.status.idle":"2021-12-06T17:02:15.224133Z","shell.execute_reply.started":"2021-12-06T17:02:14.999583Z","shell.execute_reply":"2021-12-06T17:02:15.222816Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Data instabilities noticed: </br>\n* score, ups and downs behave strangely. Score must be equal to ups - downs","metadata":{}},{"cell_type":"markdown","source":"#### 1.5 Dealing with missing data and outliers","metadata":{}},{"cell_type":"markdown","source":"Currently, this is out of scope","metadata":{}},{"cell_type":"markdown","source":"### 2. Target variable analysis","metadata":{}},{"cell_type":"code","source":"sns.countplot('label',data=train_texts)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:02:15.225396Z","iopub.execute_input":"2021-12-06T17:02:15.225666Z","iopub.status.idle":"2021-12-06T17:02:15.507217Z","shell.execute_reply.started":"2021-12-06T17:02:15.225615Z","shell.execute_reply":"2021-12-06T17:02:15.506211Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The train dataset is balanced, thus we can use accuracy score without a second thought","metadata":{}},{"cell_type":"markdown","source":"### 3. Feature Analysis","metadata":{}},{"cell_type":"markdown","source":"#### 3.1 Looking at columns and determining feature types ","metadata":{}},{"cell_type":"code","source":"train_texts.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:02:15.508680Z","iopub.execute_input":"2021-12-06T17:02:15.509308Z","iopub.status.idle":"2021-12-06T17:02:16.331819Z","shell.execute_reply.started":"2021-12-06T17:02:15.509249Z","shell.execute_reply":"2021-12-06T17:02:16.331191Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We have 10 columns: 1 label, 3 numerical (score, ups, downs) and 6 strings/timstamps. ","metadata":{}},{"cell_type":"markdown","source":"#### 3.2 Summarizing data and showing some statistics:","metadata":{}},{"cell_type":"code","source":"train_texts.describe(include=[\"object\", \"bool\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:19:13.651472Z","iopub.execute_input":"2021-12-06T17:19:13.651719Z","iopub.status.idle":"2021-12-06T17:19:19.527045Z","shell.execute_reply.started":"2021-12-06T17:19:13.651686Z","shell.execute_reply":"2021-12-06T17:19:19.525872Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Some thoughts:\n* Almost all the comments are unique\n* One third of authors are unique\n* Approximatelly 13 000 subreddits are in the dataset\n* Datasets spans 96 months = 8 years of observations","metadata":{}},{"cell_type":"code","source":"for label, dataset in train_texts.groupby('label'):\n    print(f\"\\nFor label {label} the object data statistics is:\\n\")\n    print(dataset.describe(include=['object', 'bool']))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:20:31.490285Z","iopub.execute_input":"2021-12-06T17:20:31.491008Z","iopub.status.idle":"2021-12-06T17:20:37.805324Z","shell.execute_reply.started":"2021-12-06T17:20:31.490633Z","shell.execute_reply":"2021-12-06T17:20:37.804333Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Separate statistics for 0 and 1 labels doesn't look different","metadata":{}},{"cell_type":"code","source":"train_texts.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:05:51.760625Z","iopub.execute_input":"2021-12-06T17:05:51.760921Z","iopub.status.idle":"2021-12-06T17:05:51.980166Z","shell.execute_reply.started":"2021-12-06T17:05:51.760882Z","shell.execute_reply":"2021-12-06T17:05:51.979230Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3 Analysis","metadata":{}},{"cell_type":"markdown","source":"##### 3.3.1 Comment","metadata":{}},{"cell_type":"markdown","source":"Let's add new variable which will denote the length of the comment","metadata":{}},{"cell_type":"code","source":"train_texts['length'] = [len(comment.split()) for comment in train_texts['comment']]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:28:03.841116Z","iopub.execute_input":"2021-12-06T17:28:03.841691Z","iopub.status.idle":"2021-12-06T17:28:05.191861Z","shell.execute_reply.started":"2021-12-06T17:28:03.841628Z","shell.execute_reply":"2021-12-06T17:28:05.189510Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_texts[['comment', 'length']].head()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:28:45.133411Z","iopub.execute_input":"2021-12-06T17:28:45.134010Z","iopub.status.idle":"2021-12-06T17:28:45.201742Z","shell.execute_reply.started":"2021-12-06T17:28:45.133948Z","shell.execute_reply":"2021-12-06T17:28:45.201008Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_texts['length'].describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:33:23.159615Z","iopub.execute_input":"2021-12-06T17:33:23.160167Z","iopub.status.idle":"2021-12-06T17:33:23.220771Z","shell.execute_reply.started":"2021-12-06T17:33:23.159899Z","shell.execute_reply":"2021-12-06T17:33:23.220151Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(f\"0.025 quantile of comment's length is equal to {train_texts['length'].quantile(0.025)}\")\nprint(f\"0.975 quantile of comment's length is equal to {train_texts['length'].quantile(0.975)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:40:30.179990Z","iopub.execute_input":"2021-12-06T17:40:30.180565Z","iopub.status.idle":"2021-12-06T17:40:30.211412Z","shell.execute_reply.started":"2021-12-06T17:40:30.180329Z","shell.execute_reply":"2021-12-06T17:40:30.210172Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"It looks like we have some outliers in the length of the comment column. </br>\nFor the purpose of visualization I'm going to filter out some outliers. In future whether it will be helpful for classification or not","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts = train_texts[train_texts['length'] <= train_texts['length'].quantile(0.975)] ","metadata":{"execution":{"iopub.status.busy":"2021-12-06T18:10:35.882580Z","iopub.execute_input":"2021-12-06T18:10:35.883312Z","iopub.status.idle":"2021-12-06T18:10:36.167291Z","shell.execute_reply.started":"2021-12-06T18:10:35.883260Z","shell.execute_reply":"2021-12-06T18:10:36.166185Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"Now we can compare the length distributions for 0/1 labels","metadata":{}},{"cell_type":"code","source":"g = sns.kdeplot(cleaned_train_texts[cleaned_train_texts[\"label\"]==0][\"length\"], color=\"red\", shade=True)\ng = sns.kdeplot(cleaned_train_texts[cleaned_train_texts[\"label\"]==1][\"length\"], color=\"blue\", shade=True)\ng = g.legend([\"not sarcasm\", \"sarcasm\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:44:44.501447Z","iopub.execute_input":"2021-12-06T17:44:44.501858Z","iopub.status.idle":"2021-12-06T17:44:45.004570Z","shell.execute_reply.started":"2021-12-06T17:44:44.501818Z","shell.execute_reply":"2021-12-06T17:44:45.003652Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"cleaned_train_texts[cleaned_train_texts[\"label\"]==0][\"length\"].plot.hist(bins=50, alpha = 0.5, edgecolor=\"black\", color=\"red\")\ncleaned_train_texts[cleaned_train_texts[\"label\"]==1][\"length\"].plot.hist(bins=50, alpha = 0.5, edgecolor=\"black\", color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T17:49:56.589955Z","iopub.execute_input":"2021-12-06T17:49:56.590549Z","iopub.status.idle":"2021-12-06T17:49:57.397655Z","shell.execute_reply.started":"2021-12-06T17:49:56.590502Z","shell.execute_reply":"2021-12-06T17:49:57.396665Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"'length' feature may not be the informative one. But maybe we can split it into bins - into three intervals: [1, 5), [5, 20), [21, 30] and it will improve the accuracy score.","metadata":{}},{"cell_type":"markdown","source":"##### 3.3.2 Score","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts['score'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T18:01:19.082215Z","iopub.execute_input":"2021-12-06T18:01:19.082700Z","iopub.status.idle":"2021-12-06T18:01:19.147779Z","shell.execute_reply.started":"2021-12-06T18:01:19.082625Z","shell.execute_reply":"2021-12-06T18:01:19.146936Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"'score' feature also has outliers. We ought to get rid of them","metadata":{}},{"cell_type":"code","source":"cleaned_train_texts = cleaned_train_texts[(cleaned_train_texts['score'] <= cleaned_train_texts['score'].quantile(0.975)) & \n                                  (cleaned_train_texts['score'] >= cleaned_train_texts['score'].quantile(0.025))] ","metadata":{"execution":{"iopub.status.busy":"2021-12-06T18:10:51.395914Z","iopub.execute_input":"2021-12-06T18:10:51.396277Z","iopub.status.idle":"2021-12-06T18:10:51.708389Z","shell.execute_reply.started":"2021-12-06T18:10:51.396222Z","shell.execute_reply":"2021-12-06T18:10:51.707274Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"cleaned_train_texts[(cleaned_train_texts[\"label\"]==0)][\"score\"].plot.hist(bins=20, alpha=0.5, edgecolor=\"black\", color=\"red\")\ncleaned_train_texts[(cleaned_train_texts[\"label\"]==1)][\"score\"].plot.hist(bins=20, alpha=0.5, edgecolor=\"black\", color=\"blue\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T18:12:14.353254Z","iopub.execute_input":"2021-12-06T18:12:14.353617Z","iopub.status.idle":"2021-12-06T18:12:14.989236Z","shell.execute_reply.started":"2021-12-06T18:12:14.353556Z","shell.execute_reply":"2021-12-06T18:12:14.987824Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"##### 3.3.3 Subreddits","metadata":{}},{"cell_type":"markdown","source":"'score' feature also seems to have minor effect on the value of the 'label' column","metadata":{}},{"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions","metadata":{"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"}}]}