{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Universal Sentence Encoder Multilingual model to tensorflow-lite format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade tensorflow==2.7.0 numpy==1.19.5 sentencepiece==0.1.95 scikit-learn==0.24.2 tensorflow_text==2.7.3 tqdm==4.61.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_lite_model = os.path.join(os.getcwd(), \"model.tflite\")\n",
    "\n",
    "model_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3?tf-hub-format=compressed\"\n",
    "model_name = model_url.split(\"?\")[0]\n",
    "model_name = model_name.split(\"/\")[-2] + \"_\" + model_name.split(\"/\")[-1] + \".tar.gz\"\n",
    "path_to_save = os.path.join(os.getcwd(), model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, path_to_save):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(path_to_save, 'wb') as f:\n",
    "        for chunk in tqdm(r.iter_content(chunk_size=1024), desc=f\"Downloading model from {url}\"):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def unarchive(path, path_to_extract):\n",
    "    if path.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(path, \"r:gz\")\n",
    "        tar.extractall(path=path_to_extract)\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading model\n",
    "download(model_url, path_to_save)\n",
    "saved_model_dir = path_to_save.replace(\"tar.gz\", \"\")\n",
    "#unarchiving model to use it\n",
    "unarchive(path_to_save, saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339494052"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting model to tf-lite format\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "open(path_to_save_lite_model, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tflite_model(path_to_save_model):\n",
    "    # Load the TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=path_to_save_model)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    return interpreter, input_details, output_details\n",
    "\n",
    "def tf_lite_inference(input_str, interpreter, input_details, output_details):\n",
    "    input_data = np.array([input_str])\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter, input_details, output_details = load_tflite_model(path_to_save_lite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"I'm just a text\"\n",
    "\n",
    "emb = tf_lite_inference(input_str, interpreter, input_details, output_details)\n",
    "prinft(f\"Embedding for the text {input_str} is {emb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    " - compare embeddings from tf-lite model with embeddings from regular model\n",
    " - Compare embeddings from different languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
