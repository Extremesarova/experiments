{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "First, let's import needed modules and set random seed (we'll use it if needed)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from linear_regression import SciPyLinearRegressionOLS, LinearRegressionOLS, SciPyLinearRegressionNNOLS\n",
    "from utils.scaler import StandardScaler\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading california housing dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting data into training and testing dataset (10% for test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standardize features by removing the mean and scaling to unit variance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First solution will use the analytical solution to OLS to get the weights/coefficients: $\\hat{\\beta} = (X^TX)^{-1}(X^TY)$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.82824645  0.11647235 -0.2672834   0.31051916 -0.0042315  -0.04680151\n",
      " -0.90193407 -0.87360011]\n",
      "The intercept is 2.0670505808570114\n"
     ]
    }
   ],
   "source": [
    "ols_reg = LinearRegressionOLS()\n",
    "ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {ols_reg.coef_}\")\n",
    "print(f\"The intercept is {ols_reg.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second solution is the sklearn's solution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Second' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-a6f54f1b7009>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mSecond\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Second' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "sklearn_ols_reg = LinearRegression(positive=False)\n",
    "sklearn_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {sklearn_ols_reg.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.82824645  0.11647235 -0.2672834   0.31051916 -0.0042315  -0.04680151\n",
      " -0.90193407 -0.87360011]\n",
      "The intercept is 2.067050580857012\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see the results are the same, because sklearn is using <code>scipy.linalg.lstsq</code> method under it's hood. Which, also, wrapped by me below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [ 0.82824645  0.11647235 -0.2672834   0.31051916 -0.0042315  -0.04680151\n",
      " -0.90193407 -0.87360011]\n",
      "The intercept is 2.067050580857016\n"
     ]
    }
   ],
   "source": [
    "scipy_ols_reg = SciPyLinearRegressionOLS()\n",
    "scipy_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {scipy_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {scipy_ols_reg.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are expectedly the same\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we specify <code>positive=False</code> when initializing <code>sklearn.linear_model.LinearRegression</code> object, then sklearn will use <code>scipy.optimize.nnls</code> method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [0.82413618 0.23051619 0.         0.0191287  0.0384378  0.\n",
      " 0.         0.        ]\n",
      "The intercept is 2.06705058085702\n"
     ]
    }
   ],
   "source": [
    "sklearn_ols_reg_pos = LinearRegression(positive=True)\n",
    "sklearn_ols_reg_pos.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_ols_reg_pos.coef_}\")\n",
    "print(f\"The intercept is {sklearn_ols_reg_pos.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Which is naturally equal to the results from the wrapped method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [0.82413618 0.23051619 0.         0.0191287  0.0384378  0.\n",
      " 0.         0.        ]\n",
      "The intercept is 2.06705058085704\n"
     ]
    }
   ],
   "source": [
    "scipy_nn_ols_reg = SciPyLinearRegressionNNOLS()\n",
    "scipy_nn_ols_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {scipy_nn_ols_reg.coef_}\")\n",
    "print(f\"The intercept is {scipy_nn_ols_reg.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When <code>positive</code> set to <code>True</code>, it forces the coefficients to be positive.\n",
    "\n",
    "We can see that the solution looks completely different - all the coefficients are either zero or positive.\n",
    "\n",
    "We would want to constrain the coefficients to non-negative values whenever a negative value\n",
    "makes no physical sense, say because it represents the intensity of a pixel, or the price\n",
    "of an object, or a frequency count, or a chemical concentration, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    " - the idea of regularization and why it is needed\n",
    " - using gradient descent instead of using formula (analytical solution)\n",
    " - comparing results of SGDRegressor with results of ridge regression\n",
    " - in which situations using gradient descent is more advisable\n",
    " - adjusted R2\n",
    " - Linear regression metrics\n",
    " - Non-linear methods for regression (decision trees and random forest regressors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are [-2.55186350e+01  1.15916065e+02  7.20126707e+01 -7.37100789e+01\n",
      "  1.59321093e+02 -5.68138776e+03  2.84398378e+00  6.64815807e+01]\n",
      "The intercept is [-36.38129925]\n"
     ]
    }
   ],
   "source": [
    "sklearn_sgd_reg = SGDRegressor()\n",
    "sklearn_sgd_reg.fit(X_train_scaled, y_train)\n",
    "print(f\"The coefficients are {sklearn_sgd_reg.coef_}\")\n",
    "print(f\"The intercept is {sklearn_sgd_reg.intercept_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The $R^2$ is the coefficient of determination which is defined as $R^2 = 1 -\\frac{\\sum\\limits _{i = 1} ^N (y_i - \\hat y_i)}{\\sum\\limits _{i = 1} ^N (y_i - \\bar y_i)} = 1 - \\frac{SS_{fit}}{SS_{mean}}$. <br>\n",
    "It shows how much the fit of the chosen model is better than a fit of a horizontal straight line (mean value for target $y$). <br>\n",
    "The best possible score is 1.0, and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of $y$, disregarding the input features, would get a $R^2$ score of 0.0. <br>\n",
    "$R^2$ can have a negative value without violating any rules of math. $R^2$ is negative only when the chosen model does not follow the trend of the data, so fits worse than a $y$-mean line."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 on the train data is equal to 0.61\n",
      "R2 on the test data is equal to 0.6\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "print(f\"R2 on the train data is equal to {lr.score(X_train_scaled, y_train).round(2)}\")\n",
    "print(f\"R2 on the test data is equal to {lr.score(X_test_scaled, y_test).round(2)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}