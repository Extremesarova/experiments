{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving tasks from Distributive semantics section of NLP Course from tepik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation of examples for training Word2Vec Skip Gram Negative Sampling\n",
    "\n",
    "We are training Word2Vec Skip Gram Negative Sampling with a window of a given width. For example, a window of size 5 implies that words that are no more than 2 positions to the left or right from the central word are considered positive examples. The center word is not counted as a context word.\n",
    "\n",
    "Write function, which generates training examples from the text. Every training example must look like a 3-element tuple $(CenterWord,CtxWord,Label)$, where $CenterWord∈N$ - token identifier in the middle of the window, $CtxWord∈N$ - identifier of adjacent token, $Label∈{0,1} - 1$ if $CtxWordCtxWord$ is positive and $0$, it is a negative example.\n",
    "\n",
    "Function should return the list with training examples.\n",
    "\n",
    "Arugment ns_rate sets the number of negative examples to generate for each positive example. When sampling negative words, it is usually not checked whether the word appears in the window. Thus, among negative examples, positive ones may appear.\n",
    "\n",
    "Input text was already tokenized and tokens were replaced with their identifiers.\n",
    "\n",
    "Tests are generated randomly, constraints:\n",
    "\n",
    " - len(text) < 20\n",
    " - window_size <= 11, нечётное\n",
    " - vocab_size < 100\n",
    " - ns_rate < 3\n",
    "Words have identifiers 0..vocab_size - 1 (as returns np.random.randint).\n",
    "\n",
    "NB, that -3 // 2 != -(3 // 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "def get_window(text, window_size):\n",
    "    for backward, current in enumerate(range(len(text)), start=0 - (window_size // 2)):\n",
    "        if backward < 0:\n",
    "            backward = 0\n",
    "        context = list(text[backward:current]) + list(text[current + 1:current + 1 + window_size // 2])\n",
    "        center = text[current]\n",
    "        yield center, context\n",
    "        \n",
    "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate):\n",
    "    \"\"\"\n",
    "    text - list of integer numbers - ids of tokens in text\n",
    "    window_size - odd integer - width of window\n",
    "    vocab_size - positive integer - number of tokens in vocabulary\n",
    "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
    "\n",
    "    returns list of training samples (CenterWord, CtxWord, Label)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    for center, context_values in get_window(text, window_size):\n",
    "        for context in context_values:\n",
    "            res.append([center, context, 1])\n",
    "            for n in range(ns_rate):\n",
    "                res.append([center, random.choice(np.array(range(0, vocab_size))), 0])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1], [1, 2, 0], [1, 1, 1], [1, 2, 0], [0, 1, 1], [0, 4, 0], [0, 1, 1], [0, 4, 0], [0, 0, 1], [0, 5, 0], [1, 1, 1], [1, 4, 0], [1, 0, 1], [1, 5, 0], [1, 0, 1], [1, 3, 0], [1, 0, 1], [1, 2, 0], [0, 0, 1], [0, 3, 0], [0, 1, 1], [0, 0, 0], [0, 0, 1], [0, 0, 0], [0, 5, 1], [0, 2, 0], [0, 1, 1], [0, 1, 0], [0, 0, 1], [0, 4, 0], [0, 5, 1], [0, 4, 0], [0, 0, 1], [0, 5, 0], [5, 0, 1], [5, 3, 0], [5, 0, 1], [5, 3, 0], [5, 0, 1], [5, 1, 0], [5, 3, 1], [5, 5, 0], [0, 0, 1], [0, 5, 0], [0, 5, 1], [0, 4, 0], [0, 3, 1], [0, 3, 0], [0, 5, 1], [0, 0, 0], [3, 5, 1], [3, 2, 0], [3, 0, 1], [3, 3, 0], [3, 5, 1], [3, 0, 0], [3, 5, 1], [3, 0, 0], [5, 0, 1], [5, 0, 0], [5, 3, 1], [5, 1, 0], [5, 5, 1], [5, 4, 0], [5, 3, 1], [5, 3, 0], [5, 3, 1], [5, 1, 0], [5, 5, 1], [5, 5, 0], [5, 3, 1], [5, 0, 0], [5, 0, 1], [5, 5, 0], [3, 5, 1], [3, 0, 0], [3, 5, 1], [3, 2, 0], [3, 0, 1], [3, 2, 0], [3, 5, 1], [3, 3, 0], [0, 5, 1], [0, 2, 0], [0, 3, 1], [0, 2, 0], [0, 5, 1], [0, 2, 0], [0, 0, 1], [0, 4, 0], [5, 3, 1], [5, 0, 0], [5, 0, 1], [5, 4, 0], [5, 0, 1], [5, 3, 0], [5, 5, 1], [5, 2, 0], [0, 0, 1], [0, 4, 0], [0, 5, 1], [0, 4, 0], [0, 5, 1], [0, 1, 0], [0, 2, 1], [0, 3, 0], [5, 5, 1], [5, 2, 0], [5, 0, 1], [5, 3, 0], [5, 2, 1], [5, 3, 0], [5, 0, 1], [5, 0, 0], [2, 0, 1], [2, 4, 0], [2, 5, 1], [2, 4, 0], [2, 0, 1], [2, 2, 0], [2, 1, 1], [2, 3, 0], [0, 5, 1], [0, 2, 0], [0, 2, 1], [0, 3, 0], [0, 1, 1], [0, 0, 0], [0, 3, 1], [0, 0, 0], [1, 2, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 3, 1], [1, 3, 0], [3, 0, 1], [3, 3, 0], [3, 1, 1], [3, 4, 0]]\n"
     ]
    }
   ],
   "source": [
    "text = [1, 0, 1, 0, 0, 5, 0, 3, 5, 5, 3, 0, 5, 0, 5, 2, 0, 1, 3]\n",
    "window_size = 4\n",
    "vocab_size = 6\n",
    "ns_rate = 1\n",
    "\n",
    "result = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
    "\n",
    "write_array(np.array(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
